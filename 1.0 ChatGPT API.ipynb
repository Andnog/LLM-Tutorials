{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# ChatGPT API Tutorial\n",
    "\n",
    "## General Description\n",
    "\n",
    "Welcome to the ChatGPT API Tutorial, an immersive guide to integrating **OpenAI's ChatGPT into your applications using the API with Azure OpenAI**. Whether you're a developer seeking to enhance user interactions or a data enthusiast exploring the capabilities of language models, this tutorial equips you with the knowledge to leverage ChatGPT effectively.\n",
    "\n",
    "## Agenda\n",
    "\n",
    "### Section 1: What is ChatGPT and the API\n",
    "\n",
    "- Overview of ChatGPT: Understanding its capabilities\n",
    "- Introduction to the ChatGPT API: Enabling programmatic access\n",
    "- Use cases and applications: Real-world scenarios for API integration\n",
    "\n",
    "### Section 2: How to Use the OpenAI Package for ChatGPT\n",
    "\n",
    "- Installing the OpenAI package: A step-by-step guide\n",
    "- Practical examples: Demonstrations for seamless implementation\n",
    "- Troubleshooting and FAQs: Addressing common challenges\n",
    "\n",
    "### Section 3: Hyperparameters for ChatGPT\n",
    "\n",
    "- Exploring hyperparameters: Fine-tuning model behavior\n",
    "- Temperature, max_tokens, and beyond: Customizing responses\n",
    "- Balancing creativity and specificity: Strategies for optimal configuration\n",
    "\n",
    "### Section 4: Parameters and Structure of Calling ChatGPT\n",
    "\n",
    "- Crafting effective prompts: Guidelines for meaningful interactions\n",
    "- Understanding API parameters: Tailoring requests for desired outcomes\n",
    "- Best practices for seamless communication with ChatGPT\n",
    "\n",
    "### Section 5: Models in OpenAI for ChatGPT\n",
    "\n",
    "- Model variations: Exploring available options\n",
    "- Use-case suitability: Choosing the right model for your application\n",
    "- Comparisons and recommendations: Selecting models based on requirements\n",
    "\n",
    "### Section 6: Good Practices\n",
    "\n",
    "- Formulating prompts for accuracy and clarity\n",
    "- Handling responses: Extracting valuable insights\n",
    "- Tips for optimizing performance and mitigating challenges\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Section 1: What is ChatGPT and the API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### What is ChatGPT?\n",
    "\n",
    "ChatGPT is a powerful language model developed by OpenAI, capable of generating human-like text based on given prompts. It excels in natural language understanding and can be harnessed for various applications, from content creation to conversational interfaces.\n",
    "\n",
    "ChatGPT is built upon the GPT (Generative Pre-trained Transformer) architecture, a state-of-the-art language model developed by OpenAI. GPT models are based on transformer neural networks, utilizing attention mechanisms for contextual understanding. In the case of ChatGPT, the model is specifically fine-tuned for conversational contexts.\n",
    "\n",
    "### How it work ChatGPT?\n",
    "\n",
    "Behind ChatGPT's capabilities is a vast neural network that has undergone extensive pre-training on diverse internet text. This pre-training equips the model with an inherent understanding of grammar, context, and a wide range of topics. The transformer architecture, with attention mechanisms, enables ChatGPT to grasp contextual relationships, crucial for generating coherent and contextually relevant responses.\n",
    "\n",
    "The ChatGPT API serves as the gateway to unlocking the model's potential programmatically. Through HTTP requests, users can engage with ChatGPT by providing a series of messages, each with a designated role ('system', 'user', or 'assistant') and content. The API facilitates dynamic conversations, allowing users to instruct the \n",
    "model and receive tailored responses.\n",
    "\n",
    "### Use Cases and Applications\n",
    "\n",
    "The versatility of ChatGPT makes it applicable across various domains. Explore use cases such as content creation, code generation, conversational agents, and more. Whether you're looking to enhance user interactions or automate certain tasks, ChatGPT's applications are diverse and impactful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Section 2: How to Use the OpenAI Package for ChatGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Installation Guide\n",
    "\n",
    "#### Prerequisites\n",
    "\n",
    "Before diving into using ChatGPT via the OpenAI package, ensure you have the following prerequisites installed:\n",
    "\n",
    "- Python (version 3.6 or higher)\n",
    "- OpenAI Python library (`openai`)\n",
    "\n",
    "You can install the OpenAI library using:\n",
    "\n",
    "```bash\n",
    "pip install openai\n",
    "```\n",
    "\n",
    "### Practical Examples\n",
    "\n",
    "#### Basic Usage\n",
    "\n",
    "Get started with a simple example to make your first interaction with ChatGPT:\n",
    "\n",
    "```python\n",
    "import openai\n",
    "\n",
    "openai.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
    "    ]\n",
    ")\n",
    "```\n",
    "\n",
    "This example demonstrates a basic interaction, where the system message sets the assistant's behavior, and the user message instructs the model.\n",
    "\n",
    "#### Advanced Usage with AzureOpenAI\n",
    "\n",
    "For users working with Azure, you can use the `AzureOpenAI` client. Here is an example:\n",
    "\n",
    "```python\n",
    "from azure_openai import AzureOpenAI\n",
    "import os\n",
    "\n",
    "client = AzureOpenAI(\n",
    "  api_key = os.getenv(\"AZURE_OPENAI_KEY\"),  \n",
    "  api_version = os.getenv(\"api_version\"),\n",
    "  azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"ChatGPT Response:\")\n",
    "print(response.choices[0].message.content)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "gather": {
     "logged": 1709072294975
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatGPT Response:\n",
      "The Los Angeles Dodgers won the World Series in 2020.\n"
     ]
    }
   ],
   "source": [
    "from openai import AzureOpenAI\n",
    "\n",
    "client = AzureOpenAI(\n",
    "  api_key = os.getenv(\"AZURE_OPENAI_KEY\"),  \n",
    "  api_version = os.getenv(\"api_version\"),\n",
    "  azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=os.getenv(\"model_name\"),\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"ChatGPT Response:\")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Section 3: Hyperparameters for ChatGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Hyper-parameters\n",
    "\n",
    "Hyperparameters play a crucial role in shaping the behavior of ChatGPT. Let's explore key hyperparameters and their impact on model responses:\n",
    "\n",
    "| Parameter         | Description                                             | Range            |\n",
    "|--------------------|---------------------------------------------------------|------------------|\n",
    "| temperature        | Controls the level of randomness in the generated responses. A higher value (closer to 1.0) increases creativity but may introduce randomness. | [0.0, 1.0]       |\n",
    "| max_tokens         | Sets the maximum limit of tokens in the generated response, limiting its length.  | [1, ∞]           |\n",
    "| `top_p`              | Controls the diversity of responses. A higher value (closer to 1.0) allows for a broader range of words in the output. | [0.0, 1.0]       |\n",
    "| presence_penalty   | Controls the preference for or against including certain words in the response. A higher value (closer to 1.0) encourages more mention of specified words. | [0.0, 1.0]       |\n",
    "| frequency_penalty  | Controls the preference for or against repeating words in the response. A higher value (closer to 1.0) discourages repeated words. | [0.0, 1.0]       |\n",
    "| stop_sequence      | Specifies a sequence that, when encountered, stops the generation of the response. | Any sequence     |\n",
    "| best_of            | Specifies the number of candidate responses to consider before selecting the best one. | [1, ∞]           |\n",
    "| n                  | Controls the number of responses generated for a single prompt. | [1, ∞]           |\n",
    "| log_level          | Controls the logging level of output, allowing adjustment between detailed debugging information and critical error messages. | \"debug\", \"info\", \"warning\", \"error\", \"critical\" |\n",
    "\n",
    "### Detail explanation\n",
    "\n",
    "- **Temperature**: In short, the lower the `temperature`, the more deterministic the results in the sense that the highest probable next token is always picked. Increasing temperature could lead to more randomness, which encourages more diverse or creative outputs. You are essentially increasing the weights of the other possible tokens. In terms of application, you might want to use a lower temperature value for tasks like fact-based QA to encourage more factual and concise responses. For poem generation or other creative tasks, it might be beneficial to increase the temperature value.\n",
    "\n",
    "- **Top P**: A sampling technique with `temperature`, called nucleus sampling, where you can control how deterministic the model is. If you are looking for exact and factual answers keep this low. If you are looking for more diverse responses, increase to a higher value. If you use Top P it means that only the tokens comprising the `top_p` probability mass are considered for responses, so a low `top_p` value selects the most confident responses. This means that a high `top_p` value will enable the model to look at more possible words, including less likely ones, leading to more diverse outputs. The general recommendation is to alter temperature or Top P but not both.\n",
    "\n",
    "- **Max Length**: You can manage the number of tokens the model generates by adjusting the `max length`. Specifying a max length helps you prevent long or irrelevant responses and control costs.\n",
    "\n",
    "- **Stop Sequences**: A `stop sequence` is a string that stops the model from generating tokens. Specifying stop sequences is another way to control the length and structure of the model's response. For example, you can tell the model to generate lists that have no more than 10 items by adding `\"11\"` as a stop sequence.\n",
    "\n",
    "- **Frequency Penalty**: The `frequency penalty` applies a penalty on the next token proportional to how many times that token already appeared in the response and prompt. The higher the frequency penalty, the less likely a word will appear again. This setting reduces the repetition of words in the model's response by giving tokens that appear more a higher penalty.\n",
    "\n",
    "- **Presence Penalty**: The `presence penalty` also applies a penalty on repeated tokens but, unlike the frequency penalty, the penalty is the same for all repeated tokens. A token that appears twice and a token that appears 10 times are penalized the same. This setting prevents the model from repeating phrases too often in its response. If you want the model to generate diverse or creative text, you might want to use a higher presence penalty. Or, if you need the model to stay focused, try using a lower presence penalty.\n",
    "\n",
    "Similar to `temperature` and `top_p`, the general recommendation is to alter the frequency or presence penalty but not both.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "gather": {
     "logged": 1709073337659
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "Why did the tomato turn red? Because it saw the salad dressing!\n"
     ]
    }
   ],
   "source": [
    "prompt_text = \"\"\"\n",
    "You are a helpful assistant.\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=os.getenv(\"model_name\"),\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": prompt_text},\n",
    "        {\"role\": \"user\", \"content\": \"Tell me a joke.\"},\n",
    "    ],\n",
    "    temperature=0.2,\n",
    "    max_tokens=15,\n",
    ")\n",
    "\n",
    "print(\"Output:\")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "gather": {
     "logged": 1704847743102
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "Why don't scientists trust atoms? \n",
      "\n",
      "Because they make up everything.\n"
     ]
    }
   ],
   "source": [
    "prompt_text = \"\"\"\n",
    "You are a helpful assistant.\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=os.getenv(\"model_name\"),\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": prompt_text},\n",
    "        {\"role\": \"user\", \"content\": \"Tell me a joke.\"},\n",
    "    ],\n",
    "    temperature = 0.5,\n",
    "    max_tokens = 50,\n",
    "    top_p = 0.8,\n",
    "    presence_penalty = 0.5,\n",
    "    frequency_penalty = 0.3\n",
    ")\n",
    "\n",
    "print(\"Output:\")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "gather": {
     "logged": 1704847753448
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "Why did the tomato turn red? Because it saw the salad dressing!\n",
      "Why did the tomato turn red? Because it saw the salad dressing!\n"
     ]
    }
   ],
   "source": [
    "prompt_text = \"\"\"\n",
    "You are a helpful assistant.\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=os.getenv(\"model_name\"),\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": prompt_text},\n",
    "        {\"role\": \"user\", \"content\": \"Tell me a joke.\"},\n",
    "    ],\n",
    "    temperature = 1.2,\n",
    "    n = 2,\n",
    "    top_p = 0.9,\n",
    ")\n",
    "\n",
    "print(\"Output:\")\n",
    "print(response.choices[0].message.content)\n",
    "print(response.choices[1].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Section 4: Parameters and Structure of Calling ChatGPT\n",
    "\n",
    "### Crafting Effective Prompts\n",
    "\n",
    "#### Guiding Principles\n",
    "\n",
    "- **Specificity:** Be explicit and detailed in your prompts. Clearly communicate the desired format or structure for the response.\n",
    "\n",
    "- **Contextualization:** Provide context in user messages to guide the model's understanding. System messages set the tone for the assistant's behavior.\n",
    "\n",
    "- **Iterative Conversation:** Build conversations with multiple turns. This helps the model maintain context and produce coherent responses.\n",
    "\n",
    "### Understanding API Parameters\n",
    "\n",
    "#### Model Selection\n",
    "\n",
    "- **Model:** Specify the ChatGPT model to use. For example, \"gpt-3.5-turbo.\"\n",
    "\n",
    "#### Message Structure\n",
    "\n",
    "- **Messages:** Compose an array of messages, each with a role (\"system,\" \"user,\" or \"assistant\") and content (text of the message). Order matters, with the conversation evolving sequentially.\n",
    "\n",
    "### Best Practices for Interaction\n",
    "\n",
    "#### System Message\n",
    "\n",
    "- **Role:** \"system\"\n",
    "- **Content:** Set the assistant's behavior with a concise system message at the beginning of the conversation.\n",
    "\n",
    "#### User Messages\n",
    "\n",
    "- **Role:** \"user\"\n",
    "- **Content:** Clearly instruct the model in user messages. Use detailed language and iterate the conversation for context.\n",
    "\n",
    "#### Assistant Messages\n",
    "\n",
    "- **Role:** \"assistant\"\n",
    "- **Content:** Use assistant messages to provide additional context or information to guide the model's understanding.\n",
    "\n",
    "### Troubleshooting and Common Challenges\n",
    "\n",
    "#### Unintended Output\n",
    "\n",
    "If the model provides unintended or undesirable responses:\n",
    "\n",
    "- Refine prompts: Experiment with different phrasing and provide more explicit instructions.\n",
    "  \n",
    "- Adjust hyperparameters: Fine-tune temperature, max tokens, and other settings for desired behavior.\n",
    "\n",
    "#### Response Length\n",
    "\n",
    "For controlling response length:\n",
    "\n",
    "- Utilize max tokens: Set an appropriate max tokens value to limit the length of responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "gather": {
     "logged": 1704847764823
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatGPT Response:\n",
      "In the final game, the Dodgers defeated the Tampa Bay Rays with a score of 3-1 to take their first World Series title since 1988. The game was tightly contested, with both teams playing great defense and pitching. The Dodgers took an early lead in the 1st inning with a solo home run by Mookie Betts, who was a key player throughout the playoffs. The Rays tied the game in the 4th inning, but the Dodgers quickly retook the lead with a sacrifice fly by Corey Seager in the 6th inning. Pitcher Julio Urías then came in to record the final seven outs and secure the championship for the Dodgers.\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=os.getenv(\"model_name\"),\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Tell me more about their journey to victory.\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"The Dodgers had a remarkable season, overcoming challenges and demonstrating exceptional teamwork.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What were the key highlights of the final game?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"ChatGPT Response:\")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Section 5: Models in OpenAI for ChatGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "\n",
    "\n",
    "### Latest Models in OpenAI\n",
    "\n",
    "OpenAI offers several models, each catering to specific needs. Let's explore the latest models as of the provided information:\n",
    "\n",
    "#### GPT-4 and GPT-4 Turbo\n",
    "\n",
    "- **GPT-4:** A large multimodal model with improved reasoning capabilities, broad general knowledge, and the ability to accept text or image inputs. Suitable for complex problem-solving and accurate responses.\n",
    "\n",
    "- **GPT-4 Turbo:** An optimized version of GPT-4 for chat applications, providing high accuracy and cost-effectiveness. Well-suited for traditional completions tasks using the Chat Completions API.\n",
    "\n",
    "##### GPT-4 Turbo Variants\n",
    "\n",
    "- **gpt-4-1106-preview:** The latest GPT-4 Turbo model with improved instruction following, JSON mode, reproducible outputs, parallel function calling, and more. Returns a maximum of 4,096 output tokens. Note: This preview model is not yet suited for production traffic.\n",
    "\n",
    "- **gpt-4-vision-preview:** GPT-4 Turbo with vision capabilities, enabling the understanding of images in addition to text inputs. A preview model version not yet suited for production traffic.\n",
    "\n",
    "#### GPT-3.5\n",
    "\n",
    "GPT-3.5 models, including the optimized version gpt-3.5-turbo, offer a balance of natural language understanding and generation.\n",
    "\n",
    "##### GPT-3.5 Turbo Variants\n",
    "\n",
    "- **gpt-3.5-turbo-1106:** The latest GPT-3.5 Turbo model with improved instruction following, JSON mode, reproducible outputs, parallel function calling, and more. Returns a maximum of 4,096 output tokens.\n",
    "\n",
    "- **gpt-3.5-turbo:** The optimized version for chat applications, offering a cost-effective and capable solution. Well-suited for both chat and traditional completions tasks.\n",
    "\n",
    "- **gpt-3.5-turbo-16k:** Similar to gpt-3.5-turbo but with a larger context window of 16,385 tokens.\n",
    "\n",
    "- **gpt-3.5-turbo-instruct:** Designed for compatibility with legacy Completions endpoint and not Chat Completions.\n",
    "\n",
    "### Choosing the Right Model\n",
    "\n",
    "- **Task Complexity:** For general tasks, gpt-3.5-turbo is often sufficient. GPT-4 variants excel in more complex reasoning scenarios.\n",
    "\n",
    "- **Multilingual Capabilities:** GPT-4 outperforms previous models and demonstrates strong performance in multiple languages.\n",
    "\n",
    "### Integration Example\n",
    "\n",
    "```python\n",
    "from azure_openai import AzureOpenAI\n",
    "import os\n",
    "\n",
    "client = AzureOpenAI(\n",
    "  api_key=os.getenv(\"AZURE_OPENAI_KEY\"),  \n",
    "  api_version=os.getenv(\"api_version\"),\n",
    "  azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4-1106-preview\",  # Specify the desired model\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Tell me a joke.\"},\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"ChatGPT Response:\")\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "# Note: For using other model versions it is necessary to upload the settings for the corresponding model: Endpoint and APIKey\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Section 6: Good Practices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Optimizing Interactions with ChatGPT\n",
    "\n",
    "As you engage with ChatGPT, adopting good practices can enhance the quality and effectiveness of your interactions. Let's explore key practices to optimize your experience:\n",
    "\n",
    "#### 1. Contextualize Conversations\n",
    "\n",
    "- **Why:** Provide context to guide the model's understanding.\n",
    "- **How:** Use system and user messages to build a coherent conversation. Reference prior messages for context continuity.\n",
    "\n",
    "#### 2. Experiment with Prompt Iterations\n",
    "\n",
    "- **Why:** Refine prompts for better results.\n",
    "- **How:** Experiment with different phrasing, structures, and levels of detail. Iterate to find the most effective prompt.\n",
    "\n",
    "#### 3. Control Response Length\n",
    "\n",
    "- **Why:** Manage output length for concise and relevant responses.\n",
    "- **How:** Utilize the `max_tokens` parameter to set a limit on the length of generated responses.\n",
    "\n",
    "#### 4. Fine-Tune Hyperparameters\n",
    "\n",
    "- **Why:** Adjust model behavior to meet specific requirements.\n",
    "- **How:** Experiment with hyperparameters like temperature, presence_penalty, and frequency_penalty to achieve desired response characteristics.\n",
    "\n",
    "#### 5. Utilize System Messages Effectively\n",
    "\n",
    "- **Why:** Set the behavior of the assistant for improved guidance.\n",
    "- **How:** Craft concise and clear system messages at the beginning of the conversation to influence the assistant's behavior.\n",
    "\n",
    "#### 6. Handle Unintended Output\n",
    "\n",
    "- **Why:** Address unexpected or undesirable responses.\n",
    "- **How:** Refine prompts, adjust hyperparameters, and experiment with system/user messages to improve output.\n",
    "\n",
    "### Example Integration with Good Practices\n",
    "\n",
    "```python\n",
    "from azure_openai import AzureOpenAI\n",
    "import os\n",
    "\n",
    "client = AzureOpenAI(\n",
    "  api_key=os.getenv(\"AZURE_OPENAI_KEY\"),  \n",
    "  api_version=os.getenv(\"api_version\"),\n",
    "  azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4-1106-preview\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Tell me about the impact of climate change on polar bears.\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Climate change poses significant threats to polar bears, affecting their habitat and food sources.\"},\n",
    "        {\"role\": \"user\", \"content\": \"How can we mitigate these impacts?\"},\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"ChatGPT Response:\")\n",
    "print(response.choices[0].message.content)\n",
    "```\n",
    "\n",
    "In this example, contextualizing the conversation, experimenting with prompt iterations, and utilizing effective system messages contribute to a more informative and relevant response.\n",
    "\n",
    "By incorporating these good practices, you can optimize your interactions with ChatGPT and obtain more tailored and valuable outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "gather": {
     "logged": 1704847775508
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatGPT Response:\n",
      "Mitigating the impacts of climate change on polar bears and their habitat will require significant global efforts to reduce greenhouse gas emissions and slow the rate of warming. Some other steps that can help include:\n",
      "\n",
      "1. Reducing carbon emissions: This can be achieved through a range of measures, including using more renewable energy sources, improving energy efficiency, reducing reliance on fossil fuels, and adopting sustainable transportation.\n",
      "\n",
      "2. Preserving habitats: Polar bears rely on sea ice for hunting, resting, and traveling, protecting important habitat areas can help preserve their health and safety.\n",
      "\n",
      "3. Promoting sustainable tourism: Ecotourism can provide economic incentives for conservation and can help raise public awareness about the importance of protecting polar bears and their habitat.\n",
      "\n",
      "4. Reducing waste and pollution: Reducing waste and pollution can help reduce the impact of climate change on polar bears and other species by reducing the amount of greenhouse gases released into the atmosphere.\n",
      "\n",
      "Overall, mitigation of climate change impacts on polar bears will require ongoing collaborative efforts to address the root causes of climate change and promote sustainable practices.\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=os.getenv(\"model_name\"),\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Tell me about the impact of climate change on polar bears.\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Climate change poses significant threats to polar bears, affecting their habitat and food sources.\"},\n",
    "        {\"role\": \"user\", \"content\": \"How can we mitigate these impacts?\"},\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"ChatGPT Response:\")\n",
    "print(response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python310-sdkv2"
  },
  "kernelspec": {
   "display_name": "Python 3.10 - SDK V2",
   "language": "python",
   "name": "python310-sdkv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
